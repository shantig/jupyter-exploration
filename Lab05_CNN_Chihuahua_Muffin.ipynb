{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {},
        "id": "Xa33tMJwJPAi"
      },
      "source": [
        "# Chihuahua vs Muffin Classifier using Convolutional Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PY7MJR18JPAk"
      },
      "source": [
        "# 1. Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {},
        "id": "0qoasmEdJPAk"
      },
      "source": [
        "In this lab, we'll build upon our previous workshop where we used a traditional Neural Network (NN) to classify images as either Chihuahuas or muffins. This time, we'll use a Convolutional Neural Network (CNN), which is particularly well-suited for image classification tasks because it can learn spatial hierarchies of features directly from the image data.\n",
        "By the end of this lab, we'll compare the performance of our CNN model with the traditional NN from the previous workshop.\n",
        "\n",
        "This is what we'll do in this lab:\n",
        "#### 1) Build the  convolutional neural network\n",
        "#### 2) Load the data\n",
        "#### 3) Train the model on the data\n",
        "#### 4) Visualize the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OItIaq-KJPAk"
      },
      "source": [
        "### Remember: This is an INTERACTIVE Notebook!\n",
        "You should run and play with the code as you go to see how it works. Select a cell and **press shift-enter to execute code.**\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "python\n",
        "git clone https://github.com/patitimoner/workshop-chihuahua-vs-muffin.git\n",
        "cd workshop-chihuahua-vs-muffin\n",
        "!ls"
      ],
      "metadata": {
        "id": "1bc3pspHLzeY",
        "outputId": "030d2332-2016-4509-cdcf-202a737493c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-3158993969.py, line 2)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-3158993969.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    git clone https://github.com/patitimoner/workshop-chihuahua-vs-muffin.git\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {},
        "id": "KHtA0OpdJPAl"
      },
      "source": [
        "# 2.  Setup and Imports\n",
        "\n",
        "Let's get to the fun stuff!\n",
        "First, we need to Install and  import the necessary libraries. Each import serves a specific purpose in our project.\n",
        "python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G31IlZNJJPAl",
        "outputId": "54bb7224-503c-40fc-b9b9-ae6fc4e88084",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: torch==2.8.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.8.0+cu126)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.8.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.8.0->torchvision) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch --upgrade\n",
        "!pip install torchvision --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "FmOO8jJNJPAm"
      },
      "outputs": [],
      "source": [
        "import numpy as np                          # Numpy for matrix operations\n",
        "import torch                                 # PyTorch deep learning framework\n",
        "import torch.nn as nn                        # Neural network module of PyTorch\n",
        "import torch.optim as optim                  # Optimization algorithms\n",
        "from torchvision import datasets, transforms # Tools for loading and transforming image data\n",
        "from torch.utils.data import DataLoader      # Efficient data loading\n",
        "import matplotlib.pyplot as plt              # For plotting and visualization. It is graphical library, to plot images\n",
        "# special Jupyter notebook command to show plots inline instead of in a new window\n",
        "%matplotlib inline\n",
        "from tqdm import tqdm                        # For progress bars during training\n",
        "\n",
        "# Check if CUDA is available and set the device accordingly\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {},
        "id": "GUqmMRVYJPAm"
      },
      "source": [
        "# 3 Data Preparation\n",
        "Before we start training our model, it's crucial to separate our data into training and testing (validation) sets. This separation is a fundamental concept in machine learning that helps us assess how well our model generalizes to unseen data.\n",
        "\n",
        "## 3.1 Understanding Train-Test Split\n",
        "In machine learning, we typically divide our dataset into two main subsets:\n",
        "\n",
        "1. **Training set:** This is the larger portion of the data that we use to train our model. The model learns the patterns and features from this data.\n",
        "2. **Testing set** (also called Validation set): This is a smaller portion of the data that we set aside and don't use during training. We use this to evaluate how well our model performs on unseen data.\n",
        "\n",
        "The reason for this split is to simulate how our model would perform on new, unseen data in the real world. If we tested on the same data we used for training, we wouldn't know if our model was truly learning general patterns or just memorizing the training data (a problem called overfitting).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_Zzb7aZJPAm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "print(\"Data contents:\", os.listdir(\"data\"))\n",
        "print(\"Train contents:\", os.listdir(\"data/train\"))\n",
        "print(\"Validation contents:\", os.listdir(\"data/validation\"))\n",
        "\n",
        "# Count the number of images in each set\n",
        "train_chihuahuas = len(os.listdir(\"data/train/chihuahua\"))\n",
        "train_muffins = len(os.listdir(\"data/train/muffin\"))\n",
        "val_chihuahuas = len(os.listdir(\"data/validation/chihuahua\"))\n",
        "val_muffins = len(os.listdir(\"data/validation/muffin\"))\n",
        "\n",
        "print(f\"Training set: {train_chihuahuas} Chihuahuas, {train_muffins} Muffins\")\n",
        "print(f\"Validation set: {val_chihuahuas} Chihuahuas, {val_muffins} Muffins\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {},
        "id": "3Oh7PLpCJPAn"
      },
      "source": [
        "\n",
        "## 3.2 Dataset Structure\n",
        "In our case, we've already separated our data into train and validation sets in our file structure:\n",
        "You should see that we have two main directories (same dataset as previous exercise): 'train' and 'validation', each containing subdirectories for our classes (Chihuahua and Muffin).\n",
        "\n",
        "## 3.3 Loading Separated Datasets\n",
        "Now, let's load our separated datasets:\n",
        "Remember we have to load all the images and convert them into a form that our neural network understands. Specifically, PyTorch works with **Tensor** objects. (A tensor is just a multidimensional matrix, i.e. an N-d array.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEzSZ9lFJPAn"
      },
      "source": [
        "## 3.4 Define Data transformations\n",
        "Now that we understand our dataset, let's define the transformations we'll apply to our images. These transformations help in data augmentation and normalization.\n",
        "**To easily convert our image data into tensors, we use the help of a \"dataloader.\"** The dataloader packages data into convenient boxes for our model to use. You can think of it like one person passing boxes (tensors) to another.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Rv5NpqgJPAn"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "#  Define image dimensions\n",
        "input_height, input_width = ?, ?\n",
        "\n",
        "# Define data transforms for training and validation sets\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((input_height, input_width)),  # Resize images\n",
        "        transforms.RandomHorizontalFlip(),  # Randomly flip images horizontally\n",
        "        transforms.RandomRotation(10),  # Randomly rotate images\n",
        "        transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Normalize with ImageNet mean and std\n",
        "    ]),\n",
        "    'validation': transforms.Compose([\n",
        "        transforms.Resize((input_height, input_width)),  # Resize images\n",
        "        transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Normalize with ImageNet mean and std\n",
        "    ])\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TEstPkxJPAn"
      },
      "source": [
        "## 3.5 Create Dataset and Dataloader\n",
        "With our transformations defined, we can now create our datasets and dataloaders. These will efficiently feed data into our model during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "PruvfhETJPAn"
      },
      "outputs": [],
      "source": [
        "# Load datasets\n",
        "image_datasets = {\n",
        "    'train': datasets.ImageFolder('data/train', data_transforms['train']),\n",
        "    'validation': datasets.ImageFolder('data/validation', data_transforms['validation'])\n",
        "}\n",
        "\n",
        "# Create dataloaders\n",
        "batch_size = 32\n",
        "dataloaders = {\n",
        "    'train': DataLoader(image_datasets['train'], batch_size=batch_size, shuffle=True, num_workers=4),\n",
        "    'validation': DataLoader(image_datasets['validation'], batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "}\n",
        "\n",
        "# Get the number of classes\n",
        "num_classes = len(image_datasets['train'].classes)\n",
        "print(f\"Number of classes: {num_classes}\")\n",
        "\n",
        "# Print dataset sizes\n",
        "print(f\"Training samples: {len(image_datasets['train'])}\")\n",
        "print(f\"Validation samples: {len(image_datasets['validation'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfhm7dIMJPAo"
      },
      "source": [
        "### Let's break down what this code does:\n",
        "\n",
        "1. We define separate data transforms for training and validation sets. The training set includes data augmentation (random flips and rotations) to increase variety in our training data, while the validation set doesn't use augmentation.\n",
        "\n",
        "2. We use datasets.ImageFolder to load our images from the 'train' and 'validation' directories. This function automatically assigns labels based on the subdirectory names.\n",
        "\n",
        "3. We create DataLoader objects for both sets. These handle batching our data and shuffling the training set (but not the validation set, as order doesn't matter for validation).\n",
        "\n",
        "4. Finally, we print the sizes of our datasets to confirm the split.\n",
        "\n",
        "By using separate dataloaders for training and validation, we ensure that our model is evaluated on data it hasn't seen during training, giving us a more accurate assessment of its performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Teorypu7JPAo"
      },
      "source": [
        "# 4. Model Definition\n",
        "Now that we've prepared our data, we can define our CNN model. We'll use the information about our input dimensions and number of classes to structure our network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQCr0TQHJPAo"
      },
      "outputs": [],
      "source": [
        "class ChihuahuaMuffinCNN(nn.Module):\n",
        "    def __init__(self, input_channels=3, num_classes=num_classes):\n",
        "        super(ChihuahuaMuffinCNN, self).__init__()\n",
        "\n",
        "        # Convolutional layers\n",
        "        self.features = nn.Sequential(\n",
        "            # First convolutional layer\n",
        "            nn.Conv2d(input_channels, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Second convolutional layer\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Third convolutional layer\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(128 * (input_height//8) * (input_width//8), 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)  # Pass input through convolutional layers\n",
        "        x = torch.flatten(x, 1)  # Flatten the output for the fully connected layers\n",
        "        x = self.classifier(x)  # Pass through the fully connected layers\n",
        "        return x\n",
        "\n",
        "# Initialize the model and move it to the appropriate device\n",
        "model = ChihuahuaMuffinCNN().to(device)\n",
        "print(model)\n",
        "\n",
        "# Print model summary\n",
        "from torchsummary import summary\n",
        "summary(model, (3, input_height, input_width))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hI8B0k2JPAo"
      },
      "source": [
        "# 5. Training Setup\n",
        "With our model defined, we now need to set up our loss function and optimizer. These are crucial components for training our network. In deep learning, they are the  two key components for training: Let's  recap and understand what these are and how we'll use them.\n",
        "\n",
        "**Loss Function**\n",
        "The loss function measures how well our model is performing. It calculates the difference between our model's predictions and the true labels. For our classification task, we'll use Cross Entropy Loss, which is well-suited for multi-class classification problems.\n",
        "\n",
        "**Optimizer**\n",
        "The optimizer is responsible for updating our model's parameters to minimize the loss function. We'll use the Adam optimizer, which is an extension of stochastic gradient descent (SGD) that adapts the learning rate for each parameter.\n",
        "\n",
        "Let's define our loss function and optimizer:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhVUvLi8JPAo"
      },
      "outputs": [],
      "source": [
        "# Define the loss function (Cross Entropy for classification)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the optimizer (Adam) with learning rate 0.001\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "print(\"Loss function:\", criterion)\n",
        "print(\"Optimizer:\", optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmBYJn6kJPAo"
      },
      "source": [
        "### In this code:\n",
        "\n",
        "nn.CrossEntropyLoss() creates our loss function.\n",
        "optim.Adam(model.parameters(), lr=0.001) creates our optimizer. We pass it our model's parameters and set a learning rate of 0.001."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WE00NiZ5JPAo"
      },
      "source": [
        "# 6. Model Trainning\n",
        "Now we're ready to train our model. We'll define a function to handle the training process and then run it for a specified number of epochs.\n",
        "\n",
        "This function will:\n",
        "- Iterate over our data for a specified number of epochs\n",
        "\n",
        "In each epoch, it will:\n",
        "- Train on the training data\n",
        "- Evaluate on the validation data\n",
        "\n",
        "Keep track of and print our loss and accuracy for both training and validation sets\n",
        "This will start the training process. You'll see progress bars for each epoch, along with loss and accuracy metrics for both training and validation sets.\n",
        "\n",
        "The training process may take some time, depending on your hardware. Once it's complete, we'll have a trained model ready for making predictions!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5x1edZn-JPAp"
      },
      "outputs": [],
      "source": [
        "def train_model(model, dataloaders, criterion, optimizer, num_epochs=10):\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        for phase in ['train', 'validation']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data\n",
        "            for inputs, labels in tqdm(dataloaders[phase], desc=phase):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # Zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # Backward pass + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # Statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(image_datasets[phase])\n",
        "            epoch_acc = running_corrects.double() / len(image_datasets[phase])\n",
        "\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "        print()\n",
        "\n",
        "    return model\n",
        "\n",
        "# Train the model\n",
        "trained_model = train_model(model, dataloaders, criterion, optimizer, num_epochs=10)\n",
        "\n",
        "print(\"Training complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VX3hksNAJPAp"
      },
      "source": [
        " ###This function does the following:\n",
        "\n",
        "1. For each epoch:\n",
        "-  It trains the model on the training data.\n",
        "-  It then evaluates the model on the validation data.\n",
        "-  For both phases, it calculates and prints the average loss and accuracy.\n",
        "\n",
        "\n",
        "2. The model.train() and model.eval() calls ensure the model behaves appropriately for training and validation phases.\n",
        "\n",
        "3. We use torch.set_grad_enabled() to only calculate gradients during the training phase.\n",
        "\n",
        "4. In the training phase, we perform backpropagation (loss.backward()) and update the model parameters (optimizer.step())."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {},
        "id": "-cobI9MVJPAp"
      },
      "source": [
        "# 7. Examine model performance (Model Evaluation)\n",
        "Finally, let's evaluate our trained model on the validation set and visualize some of its predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1Oq4YuoJPAp"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, dataloader):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.numpy())\n",
        "\n",
        "    return all_preds, all_labels\n",
        "\n",
        "# Get predictions\n",
        "val_preds, val_labels = evaluate_model(trained_model, dataloaders['validation'])\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = sum(np.array(val_preds) == np.array(val_labels)) / len(val_labels)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Visualize some predictions\n",
        "def plot_results(images, preds, labels):\n",
        "    fig, axs = plt.subplots(5, 6, figsize=(20, 20))\n",
        "    for i, (img_path, pred, label) in enumerate(zip(image_datasets['validation'].imgs, preds, labels)):\n",
        "        img = plt.imread(img_path[0])\n",
        "        ax = axs[i // 6, i % 6]\n",
        "        ax.imshow(img)\n",
        "        ax.axis('off')\n",
        "        color = 'green' if pred == label else 'red'\n",
        "        ax.set_title(f\"Pred: {'Chihuahua' if pred == 0 else 'Muffin'}\\nTrue: {'Chihuahua' if label == 0 else 'Muffin'}\", color=color)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_results(image_datasets['validation'].imgs, val_preds, val_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nVjzAWDJPAp"
      },
      "source": [
        "# 8.  Conclusion and Reflection\n",
        "\n",
        "Congratulations! You've successfully built, trained, and evaluated a CNN for classifying Chihuahuas and Muffins. Here are some reflection questions to consider:\n",
        "\n",
        "How does the performance of this CNN compare to the traditional Neural Network from the previous workshop?\n",
        "What role do the convolutional layers play in image classification?\n",
        "How might you further improve this model's performance?\n",
        "What challenges might this model face in real-world applications?\n",
        "How does data augmentation (like random flips and rotations) contribute to the model's performance?\n",
        "What are the ethical considerations in developing and deploying an image classification system like this?\n",
        "\n",
        "Remember to support your answers with references to relevant literature or resources on deep learning and computer vision. Good luck with your reflective journal!\n",
        "\n",
        " If you want  you can play with some hyperparameters to play with:\n",
        "- Number of epochs\n",
        "- The learning rate \"lr\" parameter in the optimizer\n",
        "- The type of optimizer (https://pytorch.org/docs/stable/optim.html)\n",
        "- Number of layers and layer dimensions\n",
        "- Image size\n",
        "- Data augmentation transforms (https://pytorch.org/docs/stable/torchvision/transforms.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {},
        "id": "U26ptzqAJPAq"
      },
      "source": [
        "# Special Thanks!\n",
        "\n",
        "Credit for the original idea and code goes to [DeepSense.ai](https://deepsense.ai/keras-vs-pytorch-avp-transfer-learning/)!\n",
        "I've modified it significantly to cater to this Lab.\n",
        "\n",
        "The original tutorial was created through hard work and love by Jing Zhao, Dylan Wang, Jason Do, Jason Jiang, and Andrew Jong."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}